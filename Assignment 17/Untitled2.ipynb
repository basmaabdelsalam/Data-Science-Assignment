{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gradient descent (SVM, Logistic Regression) give high accuracy when dataset is large\n",
    "#### KNN give high accuracy when dataset is small.\n",
    "#### Decision Tree give high accuracy when dataset is large too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(rc={'figure.figsize': [10, 10]}, font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sentiment.csv',sep='\\t',names=['Liked','Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Liked</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6913</th>\n",
       "      <td>0</td>\n",
       "      <td>Brokeback Mountain was boring.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6914</th>\n",
       "      <td>0</td>\n",
       "      <td>So Brokeback Mountain was really depressing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>0</td>\n",
       "      <td>As I sit here, watching the MTV Movie Awards, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok brokeback mountain is such a horrible movie.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>0</td>\n",
       "      <td>Oh, and Brokeback Mountain was a terrible movie.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6918 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Liked                                              Tweet\n",
       "0         1            The Da Vinci Code book is just awesome.\n",
       "1         1  this was the first clive cussler i've ever rea...\n",
       "2         1                   i liked the Da Vinci Code a lot.\n",
       "3         1                   i liked the Da Vinci Code a lot.\n",
       "4         1  I liked the Da Vinci Code but it ultimatly did...\n",
       "...     ...                                                ...\n",
       "6913      0                     Brokeback Mountain was boring.\n",
       "6914      0       So Brokeback Mountain was really depressing.\n",
       "6915      0  As I sit here, watching the MTV Movie Awards, ...\n",
       "6916      0    Ok brokeback mountain is such a horrible movie.\n",
       "6917      0   Oh, and Brokeback Mountain was a terrible movie.\n",
       "\n",
       "[6918 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"Tweet\"]\n",
    "y = df[\"Liked\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidv = TfidfVectorizer(stop_words='english')\n",
    "tfidv.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tfidv.transform(x_train)\n",
    "x_test = tfidv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5534, 1619)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>007</th>\n",
       "      <th>10</th>\n",
       "      <th>10pm</th>\n",
       "      <th>12</th>\n",
       "      <th>17</th>\n",
       "      <th>1984</th>\n",
       "      <th>1st</th>\n",
       "      <th>200</th>\n",
       "      <th>2007</th>\n",
       "      <th>286</th>\n",
       "      <th>...</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yip</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>yuck</th>\n",
       "      <th>yuh</th>\n",
       "      <th>zach</th>\n",
       "      <th>zen</th>\n",
       "      <th>µª</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5529</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5530</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5531</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5532</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5533</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5534 rows × 1619 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      007   10  10pm   12   17  1984  1st  200  2007  286  ...  yes  \\\n",
       "0     0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
       "1     0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
       "2     0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
       "3     0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
       "4     0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
       "...   ...  ...   ...  ...  ...   ...  ...  ...   ...  ...  ...  ...   \n",
       "5529  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
       "5530  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
       "5531  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
       "5532  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
       "5533  0.0  0.0   0.0  0.0  0.0   0.0  0.0  0.0   0.0  0.0  ...  0.0   \n",
       "\n",
       "      yesterday  yip  young  younger  yuck  yuh  zach  zen   µª  \n",
       "0           0.0  0.0    0.0      0.0   0.0  0.0   0.0  0.0  0.0  \n",
       "1           0.0  0.0    0.0      0.0   0.0  0.0   0.0  0.0  0.0  \n",
       "2           0.0  0.0    0.0      0.0   0.0  0.0   0.0  0.0  0.0  \n",
       "3           0.0  0.0    0.0      0.0   0.0  0.0   0.0  0.0  0.0  \n",
       "4           0.0  0.0    0.0      0.0   0.0  0.0   0.0  0.0  0.0  \n",
       "...         ...  ...    ...      ...   ...  ...   ...  ...  ...  \n",
       "5529        0.0  0.0    0.0      0.0   0.0  0.0   0.0  0.0  0.0  \n",
       "5530        0.0  0.0    0.0      0.0   0.0  0.0   0.0  0.0  0.0  \n",
       "5531        0.0  0.0    0.0      0.0   0.0  0.0   0.0  0.0  0.0  \n",
       "5532        0.0  0.0    0.0      0.0   0.0  0.0   0.0  0.0  0.0  \n",
       "5533        0.0  0.0    0.0      0.0   0.0  0.0   0.0  0.0  0.0  \n",
       "\n",
       "[5534 rows x 1619 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_train.toarray(), columns=tfidv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['007',\n",
       " '10',\n",
       " '10pm',\n",
       " '12',\n",
       " '17',\n",
       " '1984',\n",
       " '1st',\n",
       " '200',\n",
       " '2007',\n",
       " '286',\n",
       " '2nd',\n",
       " '31st',\n",
       " '33',\n",
       " '3333',\n",
       " '385',\n",
       " '50',\n",
       " '517',\n",
       " '648',\n",
       " '6th',\n",
       " '700',\n",
       " '7th',\n",
       " '8230',\n",
       " '9am',\n",
       " 'aaron',\n",
       " 'able',\n",
       " 'abortion',\n",
       " 'abrams',\n",
       " 'absolutely',\n",
       " 'absurd',\n",
       " 'academy',\n",
       " 'acceptable',\n",
       " 'accompaniment',\n",
       " 'account',\n",
       " 'achieved',\n",
       " 'aching',\n",
       " 'acne',\n",
       " 'acoustic',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'addition',\n",
       " 'admired',\n",
       " 'admiring',\n",
       " 'adorable',\n",
       " 'adore',\n",
       " 'adult',\n",
       " 'afterschool',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'ah',\n",
       " 'aka',\n",
       " 'al',\n",
       " 'allegedly',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'america',\n",
       " 'amã',\n",
       " 'anatomy',\n",
       " 'ang',\n",
       " 'angel',\n",
       " 'angels',\n",
       " 'angle',\n",
       " 'anime',\n",
       " 'aniwae',\n",
       " 'anne',\n",
       " 'answers',\n",
       " 'anyways',\n",
       " 'ap',\n",
       " 'apart',\n",
       " 'apologized',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appeals',\n",
       " 'aren',\n",
       " 'arenas',\n",
       " 'arguments',\n",
       " 'arse',\n",
       " 'artemis',\n",
       " 'article',\n",
       " 'ashamed',\n",
       " 'asian',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'asleep',\n",
       " 'ass',\n",
       " 'asshole',\n",
       " 'association',\n",
       " 'astonishingly',\n",
       " 'ate',\n",
       " 'atrocious',\n",
       " 'attempt',\n",
       " 'attractive',\n",
       " 'audrey',\n",
       " 'author',\n",
       " 'avatar',\n",
       " 'awards',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'awesomest',\n",
       " 'awful',\n",
       " 'azkaban',\n",
       " 'baby',\n",
       " 'bachelor',\n",
       " 'backdrop',\n",
       " 'background',\n",
       " 'backtory',\n",
       " 'backward',\n",
       " 'bad',\n",
       " 'badness',\n",
       " 'ball',\n",
       " 'balls',\n",
       " 'ballz',\n",
       " 'ban',\n",
       " 'bangs',\n",
       " 'barry',\n",
       " 'basically',\n",
       " 'basket',\n",
       " 'bbm',\n",
       " 'beach',\n",
       " 'beans',\n",
       " 'beat',\n",
       " 'beating',\n",
       " 'beatles',\n",
       " 'beautiful',\n",
       " 'becuase',\n",
       " 'began',\n",
       " 'begin',\n",
       " 'believe',\n",
       " 'belong',\n",
       " 'bentlys',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'better',\n",
       " 'biased',\n",
       " 'big',\n",
       " 'biggie',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'bitter',\n",
       " 'black',\n",
       " 'blame',\n",
       " 'blashpemies',\n",
       " 'blasphying',\n",
       " 'bless',\n",
       " 'blog',\n",
       " 'blonds',\n",
       " 'blood',\n",
       " 'blows',\n",
       " 'board',\n",
       " 'bobbypin',\n",
       " 'body',\n",
       " 'bogus',\n",
       " 'bolsters',\n",
       " 'bonkers',\n",
       " 'book',\n",
       " 'books',\n",
       " 'bootlegged',\n",
       " 'bored',\n",
       " 'boring',\n",
       " 'bought',\n",
       " 'bound',\n",
       " 'bout',\n",
       " 'boycott',\n",
       " 'boycotted',\n",
       " 'boycotting',\n",
       " 'boys',\n",
       " 'boyy',\n",
       " 'brazil',\n",
       " 'bridget',\n",
       " 'briefly',\n",
       " 'brigid',\n",
       " 'brilliant',\n",
       " 'bringing',\n",
       " 'bro',\n",
       " 'broke',\n",
       " 'brokeback',\n",
       " 'brooke',\n",
       " 'broom',\n",
       " 'brown',\n",
       " 'btw',\n",
       " 'budget',\n",
       " 'bullshit',\n",
       " 'bunch',\n",
       " 'burbank',\n",
       " 'burnt',\n",
       " 'butt',\n",
       " 'bye',\n",
       " 'ca',\n",
       " 'cake',\n",
       " 'calif',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'calls',\n",
       " 'came',\n",
       " 'campaign',\n",
       " 'canceled',\n",
       " 'candy',\n",
       " 'capote',\n",
       " 'care',\n",
       " 'career',\n",
       " 'carefully',\n",
       " 'caribbean',\n",
       " 'cars',\n",
       " 'casanova',\n",
       " 'case',\n",
       " 'cast',\n",
       " 'catch',\n",
       " 'catcher',\n",
       " 'category',\n",
       " 'causing',\n",
       " 'center',\n",
       " 'challenge',\n",
       " 'chamber',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changes',\n",
       " 'character',\n",
       " 'characterization',\n",
       " 'characters',\n",
       " 'cheapened',\n",
       " 'chick',\n",
       " 'chicken',\n",
       " 'childishly',\n",
       " 'children',\n",
       " 'chinese',\n",
       " 'choice',\n",
       " 'chris',\n",
       " 'christ',\n",
       " 'christain',\n",
       " 'christian',\n",
       " 'christianity',\n",
       " 'christmas',\n",
       " 'christopher',\n",
       " 'chronicles',\n",
       " 'chronological',\n",
       " 'cinema',\n",
       " 'claiming',\n",
       " 'class',\n",
       " 'classes',\n",
       " 'classic',\n",
       " 'clean',\n",
       " 'cleaning',\n",
       " 'clearly',\n",
       " 'clive',\n",
       " 'cloak',\n",
       " 'clothed',\n",
       " 'club',\n",
       " 'clubbin',\n",
       " 'cobequid',\n",
       " 'cock',\n",
       " 'cocktail',\n",
       " 'code',\n",
       " 'coherent',\n",
       " 'cold',\n",
       " 'colfer',\n",
       " 'collection',\n",
       " 'college',\n",
       " 'colony',\n",
       " 'colourfully',\n",
       " 'combining',\n",
       " 'combonation',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'comment',\n",
       " 'comments',\n",
       " 'community',\n",
       " 'compared',\n",
       " 'comparsions',\n",
       " 'complaints',\n",
       " 'completely',\n",
       " 'complex',\n",
       " 'conclusion',\n",
       " 'condeming',\n",
       " 'condemnation',\n",
       " 'confess',\n",
       " 'congrats',\n",
       " 'conquering',\n",
       " 'consider',\n",
       " 'considered',\n",
       " 'consumed',\n",
       " 'contains',\n",
       " 'controversy',\n",
       " 'conversations',\n",
       " 'convo',\n",
       " 'cool',\n",
       " 'copy',\n",
       " 'corrupting',\n",
       " 'costumes',\n",
       " 'count',\n",
       " 'counting',\n",
       " 'course',\n",
       " 'cover',\n",
       " 'cow',\n",
       " 'cowan',\n",
       " 'cowboy',\n",
       " 'cowboys',\n",
       " 'coz',\n",
       " 'crafted',\n",
       " 'crap',\n",
       " 'crappy',\n",
       " 'crash',\n",
       " 'crazy',\n",
       " 'created',\n",
       " 'creature',\n",
       " 'credit',\n",
       " 'creed',\n",
       " 'cried',\n",
       " 'cringe',\n",
       " 'criticized',\n",
       " 'criticizers',\n",
       " 'critics',\n",
       " 'cruise',\n",
       " 'crusade',\n",
       " 'crystal',\n",
       " 'cucumber',\n",
       " 'culture',\n",
       " 'cussler',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'cuz',\n",
       " 'da',\n",
       " 'dad',\n",
       " 'daddy',\n",
       " 'dakota',\n",
       " 'damn',\n",
       " 'dan',\n",
       " 'dance',\n",
       " 'daniel',\n",
       " 'danielle',\n",
       " 'darkness',\n",
       " 'dart',\n",
       " 'dash',\n",
       " 'date',\n",
       " 'dating',\n",
       " 'davinci',\n",
       " 'day',\n",
       " 'days',\n",
       " 'deals',\n",
       " 'dearly',\n",
       " 'death',\n",
       " 'debates',\n",
       " 'decaying',\n",
       " 'decent',\n",
       " 'decide',\n",
       " 'decided',\n",
       " 'deciding',\n",
       " 'decline',\n",
       " 'decomposing',\n",
       " 'dedicated',\n",
       " 'deemed',\n",
       " 'deep',\n",
       " 'defensive',\n",
       " 'definately',\n",
       " 'definitely',\n",
       " 'degraw',\n",
       " 'delicious',\n",
       " 'demeantor',\n",
       " 'dementors',\n",
       " 'demons',\n",
       " 'depp',\n",
       " 'depressing',\n",
       " 'depth',\n",
       " 'derek',\n",
       " 'deserved',\n",
       " 'desperate',\n",
       " 'desperately',\n",
       " 'despise',\n",
       " 'despised',\n",
       " 'devastate',\n",
       " 'devil',\n",
       " 'diana',\n",
       " 'dick',\n",
       " 'dictate',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'didnt',\n",
       " 'didnâ',\n",
       " 'die',\n",
       " 'died',\n",
       " 'dies',\n",
       " 'different',\n",
       " 'differently',\n",
       " 'dinner',\n",
       " 'directed',\n",
       " 'director',\n",
       " 'disappointed',\n",
       " 'disappointing',\n",
       " 'discovered',\n",
       " 'discuss',\n",
       " 'discussed',\n",
       " 'discussing',\n",
       " 'dislike',\n",
       " 'disliked',\n",
       " 'disney',\n",
       " 'disruption',\n",
       " 'dissapointed',\n",
       " 'diversity',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'dogfucking',\n",
       " 'dogtown',\n",
       " 'doing',\n",
       " 'don',\n",
       " 'donkey',\n",
       " 'dont',\n",
       " 'donâ',\n",
       " 'doors',\n",
       " 'dork',\n",
       " 'dorks',\n",
       " 'draco',\n",
       " 'dragged',\n",
       " 'dragons',\n",
       " 'drain',\n",
       " 'draw',\n",
       " 'drawing',\n",
       " 'drawn',\n",
       " 'dream',\n",
       " 'dress',\n",
       " 'drive',\n",
       " 'dudeee',\n",
       " 'dumb',\n",
       " 'dumbest',\n",
       " 'dumbledor',\n",
       " 'durno',\n",
       " 'dvds',\n",
       " 'dynamite',\n",
       " 'earrings',\n",
       " 'eating',\n",
       " 'edition',\n",
       " 'editor',\n",
       " 'education',\n",
       " 'eek',\n",
       " 'effects',\n",
       " 'egg',\n",
       " 'eh',\n",
       " 'emily',\n",
       " 'emma',\n",
       " 'emo',\n",
       " 'encourage',\n",
       " 'end',\n",
       " 'ending',\n",
       " 'energy',\n",
       " 'england',\n",
       " 'english',\n",
       " 'enjoy',\n",
       " 'enjoyed',\n",
       " 'enjoying',\n",
       " 'enjoyment',\n",
       " 'enjoys',\n",
       " 'ennis',\n",
       " 'entire',\n",
       " 'entitled',\n",
       " 'eoin',\n",
       " 'equal',\n",
       " 'equally',\n",
       " 'equus',\n",
       " 'eragon',\n",
       " 'erm',\n",
       " 'escapades',\n",
       " 'especially',\n",
       " 'esther',\n",
       " 'et',\n",
       " 'events',\n",
       " 'everybody',\n",
       " 'evil',\n",
       " 'evilpinkmunky',\n",
       " 'ew',\n",
       " 'example',\n",
       " 'excellent',\n",
       " 'excited',\n",
       " 'exciting',\n",
       " 'executed',\n",
       " 'exelent',\n",
       " 'exhausted',\n",
       " 'expected',\n",
       " 'explaination',\n",
       " 'explains',\n",
       " 'explore',\n",
       " 'explosions',\n",
       " 'expo',\n",
       " 'exponentially',\n",
       " 'exquisite',\n",
       " 'extent',\n",
       " 'extremely',\n",
       " 'eye',\n",
       " 'eyre',\n",
       " 'fabricated',\n",
       " 'fabulous',\n",
       " 'facebook',\n",
       " 'facile',\n",
       " 'facing',\n",
       " 'fact',\n",
       " 'fade',\n",
       " 'fair',\n",
       " 'fairly',\n",
       " 'fall',\n",
       " 'fallon',\n",
       " 'families',\n",
       " 'fan',\n",
       " 'fandom',\n",
       " 'fanfic',\n",
       " 'fanfiction',\n",
       " 'fantasy',\n",
       " 'far',\n",
       " 'fat',\n",
       " 'fault',\n",
       " 'favor',\n",
       " 'favorite',\n",
       " 'favourite',\n",
       " 'feast',\n",
       " 'feathers',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'felicia',\n",
       " 'fell',\n",
       " 'felt',\n",
       " 'festivities',\n",
       " 'fiber',\n",
       " 'fic',\n",
       " 'field',\n",
       " 'figure',\n",
       " 'figured',\n",
       " 'figures',\n",
       " 'film',\n",
       " 'films',\n",
       " 'final',\n",
       " 'finale',\n",
       " 'finally',\n",
       " 'finals',\n",
       " 'finish',\n",
       " 'finished',\n",
       " 'finshed',\n",
       " 'fireworks',\n",
       " 'fit',\n",
       " 'fits',\n",
       " 'flat',\n",
       " 'flick',\n",
       " 'folk',\n",
       " 'folows',\n",
       " 'food',\n",
       " 'forever',\n",
       " 'forget',\n",
       " 'forgotten',\n",
       " 'form',\n",
       " 'formed',\n",
       " 'fowl',\n",
       " 'fr',\n",
       " 'frakking',\n",
       " 'france',\n",
       " 'franchise',\n",
       " 'freagin',\n",
       " 'freak',\n",
       " 'freakin',\n",
       " 'freaking',\n",
       " 'free',\n",
       " 'freezing',\n",
       " 'frenzied',\n",
       " 'friday',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'friendships',\n",
       " 'friggin',\n",
       " 'frodo',\n",
       " 'fuck',\n",
       " 'fucking',\n",
       " 'fully',\n",
       " 'fun',\n",
       " 'funniest',\n",
       " 'funny',\n",
       " 'futile',\n",
       " 'future',\n",
       " 'fyi',\n",
       " 'gadgets',\n",
       " 'gaither',\n",
       " 'game',\n",
       " 'garrett',\n",
       " 'gary',\n",
       " 'gathered',\n",
       " 'gavin',\n",
       " 'gay',\n",
       " 'gayer',\n",
       " 'gayness',\n",
       " 'gays',\n",
       " 'geek',\n",
       " 'generalized',\n",
       " 'generally',\n",
       " 'generation',\n",
       " 'genre',\n",
       " 'genres',\n",
       " 'georgia',\n",
       " 'getting',\n",
       " 'gettting',\n",
       " 'giants',\n",
       " 'gift',\n",
       " 'gin',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'given',\n",
       " 'gl',\n",
       " 'glad',\n",
       " 'glitz',\n",
       " 'gn',\n",
       " 'goblet',\n",
       " 'god',\n",
       " 'goes',\n",
       " 'goin',\n",
       " 'going',\n",
       " 'gonna',\n",
       " 'good',\n",
       " 'google',\n",
       " 'gorgeous',\n",
       " 'gossip',\n",
       " 'got',\n",
       " 'goth',\n",
       " 'gotta',\n",
       " 'grabs',\n",
       " 'grand',\n",
       " 'great',\n",
       " 'grey',\n",
       " 'groaning',\n",
       " 'ground',\n",
       " 'group',\n",
       " 'grow',\n",
       " 'grown',\n",
       " 'gun',\n",
       " 'guts',\n",
       " 'guy',\n",
       " 'guys',\n",
       " 'gyllenhaal',\n",
       " 'gym',\n",
       " 'haha',\n",
       " 'hahaha',\n",
       " 'hahahaha',\n",
       " 'hahahahahaha',\n",
       " 'hahash',\n",
       " 'haircut',\n",
       " 'hairy',\n",
       " 'half',\n",
       " 'halfway',\n",
       " 'hall',\n",
       " 'halle',\n",
       " 'halloween',\n",
       " 'halls',\n",
       " 'hammy',\n",
       " 'hand',\n",
       " 'hands',\n",
       " 'hanging',\n",
       " 'hank',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happiness',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hardcore',\n",
       " 'harrison',\n",
       " 'harry',\n",
       " 'hat',\n",
       " 'hate',\n",
       " 'hated',\n",
       " 'hates',\n",
       " 'hating',\n",
       " 'haunt',\n",
       " 'haunted',\n",
       " 'haven',\n",
       " 'having',\n",
       " 'head',\n",
       " 'health',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'heartbraking',\n",
       " 'heath',\n",
       " 'heather',\n",
       " 'heavy',\n",
       " 'hedge',\n",
       " 'hell',\n",
       " 'hella',\n",
       " 'hello',\n",
       " 'helped',\n",
       " 'heresies',\n",
       " 'hermione',\n",
       " 'hero',\n",
       " 'het',\n",
       " 'heteronormativity',\n",
       " 'hey',\n",
       " 'hide',\n",
       " 'hilarious',\n",
       " 'hill',\n",
       " 'hippie',\n",
       " 'hips',\n",
       " 'historical',\n",
       " 'hoffman',\n",
       " 'hogwarts',\n",
       " 'hogwash',\n",
       " 'hold',\n",
       " 'holding',\n",
       " 'hollywood',\n",
       " 'hollywoord',\n",
       " 'holy',\n",
       " 'home',\n",
       " 'homophobes',\n",
       " 'homophobic',\n",
       " 'homosexuality',\n",
       " 'honestly',\n",
       " 'honor',\n",
       " 'hooked',\n",
       " 'hooker',\n",
       " 'hookup',\n",
       " 'hoot',\n",
       " 'hoover',\n",
       " 'hope',\n",
       " 'hopefully',\n",
       " 'horrible',\n",
       " 'horses',\n",
       " 'hot',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'howard',\n",
       " 'hp',\n",
       " 'huge',\n",
       " 'hugged',\n",
       " 'hugh',\n",
       " 'huh',\n",
       " 'humor',\n",
       " 'hung',\n",
       " 'hype',\n",
       " 'hyped',\n",
       " 'icons',\n",
       " 'idea',\n",
       " 'ideas',\n",
       " 'idiots',\n",
       " 'idk',\n",
       " 'iii',\n",
       " 'illegally',\n",
       " 'illustrated',\n",
       " 'im',\n",
       " 'image',\n",
       " 'imagine',\n",
       " 'immediately',\n",
       " 'immensely',\n",
       " 'immortal',\n",
       " 'imo',\n",
       " 'important',\n",
       " 'impossible',\n",
       " 'impressive',\n",
       " 'inaccurate',\n",
       " 'included',\n",
       " 'including',\n",
       " 'increasing',\n",
       " 'incredible',\n",
       " 'incredibly',\n",
       " 'independant',\n",
       " 'independent',\n",
       " 'indicative',\n",
       " 'indoctrinate',\n",
       " 'industry',\n",
       " 'infiltrate',\n",
       " 'inherently',\n",
       " 'insane',\n",
       " 'insanely',\n",
       " 'inside',\n",
       " 'inspired',\n",
       " 'instead',\n",
       " 'insurance',\n",
       " 'intellectual',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'interview',\n",
       " 'intrigued',\n",
       " 'invisibility',\n",
       " 'invisible',\n",
       " 'involve',\n",
       " 'iq',\n",
       " 'irrespective',\n",
       " 'isn',\n",
       " 'issues',\n",
       " 'ive',\n",
       " 'jack',\n",
       " 'jackson',\n",
       " 'jake',\n",
       " 'jamaica',\n",
       " 'jame',\n",
       " 'jamie',\n",
       " 'jane',\n",
       " 'japenese',\n",
       " 'jay',\n",
       " 'jelly',\n",
       " 'jenn',\n",
       " 'jessica',\n",
       " 'jesus',\n",
       " 'jill',\n",
       " 'job',\n",
       " 'joe',\n",
       " 'john',\n",
       " 'johnny',\n",
       " 'joiners',\n",
       " 'joining',\n",
       " 'joke',\n",
       " 'jokes',\n",
       " 'jon',\n",
       " 'jones',\n",
       " 'josie',\n",
       " 'joy',\n",
       " 'judgement',\n",
       " 'juicy',\n",
       " 'julia',\n",
       " 'just',\n",
       " 'kaka',\n",
       " 'kanye',\n",
       " 'kat',\n",
       " 'kate',\n",
       " 'kelse',\n",
       " 'kelsie',\n",
       " 'kept',\n",
       " 'keys',\n",
       " 'kick',\n",
       " 'kicked',\n",
       " 'kid',\n",
       " 'kids',\n",
       " 'kill',\n",
       " 'kind',\n",
       " 'kinda',\n",
       " 'kirsten',\n",
       " 'kiss',\n",
       " 'knew',\n",
       " 'knight',\n",
       " 'knights',\n",
       " 'know',\n",
       " 'knows',\n",
       " 'kudos',\n",
       " 'la',\n",
       " 'laid',\n",
       " 'lama',\n",
       " 'lame',\n",
       " 'lamest',\n",
       " 'lapse',\n",
       " 'larry',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'latest',\n",
       " 'latin',\n",
       " 'laughed',\n",
       " 'laura',\n",
       " 'lazy',\n",
       " 'leah',\n",
       " 'learn',\n",
       " 'leder',\n",
       " 'lee',\n",
       " 'left',\n",
       " 'legacy',\n",
       " 'lends',\n",
       " 'let',\n",
       " 'lets',\n",
       " 'letter',\n",
       " 'letting',\n",
       " 'level',\n",
       " 'libraries',\n",
       " 'libre',\n",
       " 'lie',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likes',\n",
       " 'lil',\n",
       " 'lines',\n",
       " 'linked',\n",
       " 'listen',\n",
       " 'listens',\n",
       " 'lit',\n",
       " 'literature',\n",
       " 'little',\n",
       " 'live',\n",
       " 'livejournal',\n",
       " 'lives',\n",
       " 'living',\n",
       " 'll',\n",
       " 'loathe',\n",
       " 'lol',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'looked',\n",
       " 'looks',\n",
       " 'lord',\n",
       " 'lords',\n",
       " 'lore',\n",
       " 'loser',\n",
       " 'lost',\n",
       " 'lot',\n",
       " 'lotr',\n",
       " 'loudest',\n",
       " 'lousy',\n",
       " 'love',\n",
       " 'loved',\n",
       " 'loves',\n",
       " 'low',\n",
       " 'lower',\n",
       " 'luau',\n",
       " 'lubb',\n",
       " 'luck',\n",
       " 'lucky',\n",
       " 'lunch',\n",
       " 'luv',\n",
       " 'lynn',\n",
       " 'lynne',\n",
       " 'machine',\n",
       " 'madly',\n",
       " 'madre',\n",
       " 'magic',\n",
       " 'magical',\n",
       " 'main',\n",
       " 'mainstream',\n",
       " 'major',\n",
       " 'majorly',\n",
       " 'make',\n",
       " 'making',\n",
       " 'malaguena',\n",
       " 'malfoy',\n",
       " 'mall',\n",
       " 'man',\n",
       " 'mang',\n",
       " 'manga',\n",
       " 'march',\n",
       " 'marcia',\n",
       " 'margaritas',\n",
       " 'marisa',\n",
       " 'master',\n",
       " 'match',\n",
       " 'material',\n",
       " 'matter',\n",
       " 'maybe',\n",
       " 'mcgarther',\n",
       " 'mcmurtry',\n",
       " 'mcphee',\n",
       " 'mean',\n",
       " 'measure',\n",
       " 'meat',\n",
       " 'media',\n",
       " 'meeting',\n",
       " 'melbourne',\n",
       " 'men',\n",
       " 'mention',\n",
       " 'mentioned',\n",
       " 'messiah',\n",
       " 'messy',\n",
       " 'mi',\n",
       " 'mi3',\n",
       " 'microsoft',\n",
       " 'million',\n",
       " 'min',\n",
       " 'mind',\n",
       " 'mindedness',\n",
       " 'minutes',\n",
       " 'mirror',\n",
       " 'miss',\n",
       " 'mission',\n",
       " 'mob',\n",
       " 'mocking',\n",
       " 'moives',\n",
       " 'mom',\n",
       " 'moments',\n",
       " 'monchel',\n",
       " 'money',\n",
       " 'monsters',\n",
       " 'mood',\n",
       " 'moralistic',\n",
       " 'mormon',\n",
       " 'mother',\n",
       " 'mound',\n",
       " 'mountain',\n",
       " 'mouth',\n",
       " 'movie',\n",
       " 'movies',\n",
       " 'moving',\n",
       " 'mph',\n",
       " 'mrs',\n",
       " 'mtv',\n",
       " 'muahahaahahah',\n",
       " 'murderball',\n",
       " 'music',\n",
       " 'musiclove',\n",
       " 'mybutthole',\n",
       " 'nacho',\n",
       " 'nanny',\n",
       " 'napoleon',\n",
       " 'narnia',\n",
       " 'nasy',\n",
       " 'nc',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'need',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.976878612716763"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4339    0\n",
       "6103    0\n",
       "5457    0\n",
       "5021    0\n",
       "4062    0\n",
       "       ..\n",
       "4335    0\n",
       "5535    0\n",
       "6828    0\n",
       "6071    0\n",
       "6244    0\n",
       "Name: Liked, Length: 1384, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[593,   5],\n",
       "       [  2, 784]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994942196531792"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4339    0\n",
       "6103    0\n",
       "5457    0\n",
       "5021    0\n",
       "4062    0\n",
       "       ..\n",
       "4335    0\n",
       "5535    0\n",
       "6828    0\n",
       "6071    0\n",
       "6244    0\n",
       "Name: Liked, Length: 1384, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[591,   7],\n",
       "       [  2, 784]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9934971098265896"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4339    0\n",
       "6103    0\n",
       "5457    0\n",
       "5021    0\n",
       "4062    0\n",
       "       ..\n",
       "4335    0\n",
       "5535    0\n",
       "6828    0\n",
       "6071    0\n",
       "6244    0\n",
       "Name: Liked, Length: 1384, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[583,  15],\n",
       "       [  7, 779]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9841040462427746"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
